{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c5442c",
   "metadata": {},
   "source": [
    "Question 1\n",
    "\n",
    "P(rainy) = 1/2\n",
    "\n",
    "P(sunny) = 1/2\n",
    "\n",
    "P(sunny|a cone of ice cream) = P(sunny) * P(a|sunny) * P(cone|sunny) * P(of|sunny) * P(ice|sunny) * P(cream|sunny)\n",
    "\n",
    "P(rainy|a cup of hot coffee) = P(rainy) * P(a|rainy) * P(cup|rainy) * P(of|rainy) * P(hot|rainy) * P(coffee|rainy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8945ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import classificationMethod\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ea2e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier(classificationMethod.ClassificationMethod):\n",
    "\n",
    "    def __init__(self, legalLabels):\n",
    "        self.legalLabels = legalLabels\n",
    "        self.type = \"naivebayes\"\n",
    "        self.k = 1  # this is the smoothing parameter, ** use it in your train method **\n",
    "        self.automaticTuning = False  # Look at this flag to decide whether to choose k automatically ** use this in your train method **\n",
    "\n",
    "    def setSmoothing(self, k):\n",
    "        # This is used by the main method to change the smoothing parameter before training.\n",
    "        # Do not modify this method.\n",
    "        self.k = k\n",
    "\n",
    "    def train(self, trainingData, trainingLabels, validationData, validationLabels):\n",
    "        self.features = list(trainingData[0].keys())  # this could be useful for your code later...\n",
    "        if self.automaticTuning:\n",
    "            kgrid = [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 50]\n",
    "        else:\n",
    "            kgrid = [self.k]\n",
    "        self.trainAndTune(trainingData, trainingLabels, validationData, validationLabels, kgrid)\n",
    "\n",
    "    def trainAndTune(self, trainingData, trainingLabels, validationData, validationLabels, kgrid):\n",
    "        bestAccuracyCount = -1  # best accuracy so far on validation set\n",
    "        # Common training - get all counts from training data\n",
    "        # We only do it once - save computation in tuning smoothing parameter\n",
    "        commonPrior = util.Counter()  # probability over labels\n",
    "        commonConditionalProb = util.Counter()  # Conditional probability of feature feat being 1\n",
    "        # indexed by (feat, label)\n",
    "        commonCounts = util.Counter()  # how many time I have seen feature feat with label y\n",
    "        # whatever inactive or active\n",
    "        for i in range(len(trainingData)):\n",
    "            datum = trainingData[i]\n",
    "            label = trainingLabels[i]\n",
    "            commonPrior[label] += 1\n",
    "            for feat, value in datum.items():\n",
    "                commonCounts[(feat, label)] += 1\n",
    "                if value > 0:  # assume binary value\n",
    "                    commonConditionalProb[(feat, label)] += 1\n",
    "        for k in kgrid:  # Smoothing parameter tuning loop!\n",
    "            prior = util.Counter()\n",
    "            conditionalProb = util.Counter()\n",
    "            counts = util.Counter()\n",
    "            # get counts from common training step\n",
    "            for key, val in commonPrior.items():\n",
    "                prior[key] += val\n",
    "            for key, val in commonCounts.items():\n",
    "                counts[key] += val\n",
    "            for key, val in commonConditionalProb.items():\n",
    "                conditionalProb[key] += val\n",
    "            # smoothing:\n",
    "            for label in self.legalLabels:\n",
    "                for feat in self.features:\n",
    "                    conditionalProb[(feat, label)] += k\n",
    "                    counts[(feat, label)] += 2 * k  # 2 because both value 0 and 1 are smoothed\n",
    "            # normalizing:\n",
    "            prior.normalize()\n",
    "            for x, count in conditionalProb.items():\n",
    "                conditionalProb[x] = count * 1.0 / counts[x]\n",
    "            self.prior = prior\n",
    "            self.conditionalProb = conditionalProb\n",
    "            # evaluating performance on validation set\n",
    "            predictions = self.classify(validationData)\n",
    "            accuracyCount = [predictions[i] == validationLabels[i] for i in range(len(validationLabels))].count(True)\n",
    "            print(\"Performance on validation set for k=%f: (%.1f%%)\" % (k, 100.0 * accuracyCount / len(validationLabels)))\n",
    "            if accuracyCount > bestAccuracyCount:\n",
    "                bestParams = (prior, conditionalProb, k)\n",
    "                bestAccuracyCount = accuracyCount\n",
    "        # end of automatic tuning loop\n",
    "        self.prior, self.conditionalProb, self.k = bestParams\n",
    "        util.raiseNotDefined()\n",
    "        \n",
    "        #https://github.com/sabotuer99/edx_Artificial_Intelligence/blob/master/p5/classification/naiveBayes.py\n",
    "\n",
    "    def classify(self, testData):\n",
    "        guesses = []\n",
    "        self.posteriors = []  # Log posteriors are stored for later data analysis (autograder).\n",
    "        for datum in testData:\n",
    "            posterior = self.calculateLogJointProbabilities(datum)\n",
    "            guesses.append(posterior.argMax())\n",
    "            self.posteriors.append(posterior)\n",
    "        return guesses\n",
    "\n",
    "    def calculateLogJointProbabilities(self, datum):\n",
    "        logJoint = util.Counter()\n",
    "        for label in self.legalLabels:\n",
    "            logJoint[label] = math.log(self.prior[label])\n",
    "            for feat, value in datum.items():\n",
    "                if value > 0:\n",
    "                    logJoint[label] += math.log(self.conditionalProb[feat, label])\n",
    "                else:\n",
    "                    logJoint[label] += math.log(1 - self.conditionalProb[feat, label])\n",
    "        util.raiseNotDefined()\n",
    "        return logJoint\n",
    "     \n",
    "    #https://github.com/sabotuer99/edx_Artificial_Intelligence/blob/master/p5/classification/naiveBayes.py\n",
    "    \n",
    "    def findHighOddsFeatures(self, label1, label2):\n",
    "        featuresOdds = []\n",
    "        for feat in self.features:\n",
    "            featuresOdds.append((self.conditionalProb[feat, label1] / self.conditionalProb[feat, label2], feat))\n",
    "            featuresOdds.sort()\n",
    "            featuresOdds = [feat for val, feat in featuresOdds[-100:]]\n",
    "        util.raiseNotDefined()\n",
    "        return featuresOdds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff966185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing classification\n",
      "--------------------\n",
      "data:\t\tdigits\n",
      "classifier:\t\tmostFrequent\n",
      "training set size:\t100\n",
      "Extracting features...\n",
      "Training...\n",
      "Validating...\n",
      "126 correct out of 1000 (12.6%).\n",
      "Testing...\n",
      "108 correct out of 1000 (10.8%).\n",
      "===================================\n",
      "Mistake on example 0\n",
      "Predicted 1; truth is 9\n",
      "Image: \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "             ++###+         \n",
      "             ######+        \n",
      "            +######+        \n",
      "            ##+++##+        \n",
      "           +#+  +##+        \n",
      "           +##++###+        \n",
      "           +#######+        \n",
      "           +#######+        \n",
      "            +##+###         \n",
      "              ++##+         \n",
      "              +##+          \n",
      "              ###+          \n",
      "            +###+           \n",
      "            +##+            \n",
      "           +##+             \n",
      "          +##+              \n",
      "         +##+               \n",
      "         ##+                \n",
      "        +#+                 \n",
      "        +#+                 \n",
      "                            \n"
     ]
    }
   ],
   "source": [
    "!python dataClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26593103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: \n",
      "  USAGE:      python dataClassifier.py <options>\n",
      "  EXAMPLES:   (1) python dataClassifier.py\n",
      "                  - trains the default mostFrequent classifier on the digit dataset\n",
      "                  using the default 100 training examples and\n",
      "                  then test the classifier on test data\n",
      "                 \n",
      "\n",
      "Options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -c CLASSIFIER, --classifier=CLASSIFIER\n",
      "                        The type of classifier [Default: mostFrequent]\n",
      "  -d DATA, --data=DATA  Dataset to use [Default: digits]\n",
      "  -t TRAINING, --training=TRAINING\n",
      "                        The size of the training set [Default: 100]\n",
      "  -a, --autotune        Whether to automatically tune hyperparameters\n",
      "                        [Default: False]\n",
      "  -i ITERATIONS, --iterations=ITERATIONS\n",
      "                        Maximum iterations to run training [Default: 3]\n"
     ]
    }
   ],
   "source": [
    "!python dataClassifier.py -h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43fd3aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing classification\n",
      "--------------------\n",
      "data:\t\tdigits\n",
      "classifier:\t\tnaiveBayes\n",
      "training set size:\t100\n",
      "using automatic tuning for naivebayes\n",
      "Extracting features...\n",
      "Training...\n",
      "Method not implemented: trainAndTune\n"
     ]
    }
   ],
   "source": [
    "!python dataClassifier.py -c naiveBayes --autotune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d363b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "\n",
    "#https://github.com/sabotuer99/edx_Artificial_Intelligence/blob/master/p5/classification/naiveBayes.py\n",
    "#https://www.youtube.com/watch?v=grHXPlwtNQY&list=PLiWNvnK7PSPE--36RIdeHg8Sgg02w9ch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
